#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
BOSSç›´è˜æ‰¹é‡æ•°æ®é‡‡é›†è„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰
æ”¹è¿›ç‚¹ï¼š
1. è¾¹é‡‡é›†è¾¹ä¿å­˜ï¼ŒCtrl+Cä¸ä¼šä¸¢å¤±æ•°æ®
2. ä½¿ç”¨APIè·å–è¯¦æƒ…ï¼Œé¿å…è·³è½¬é¡µé¢
3. å¢åŠ æ›´é•¿çš„å»¶è¿Ÿæ—¶é—´
4. å®æ—¶ä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

from DrissionPage import ChromiumPage
from DrissionPage.common import Settings
import csv
import time
import re
import random
import os
from datetime import datetime
from collections import defaultdict
import signal
import sys

# è®¾ç½®å…è®¸å¤šå¯¹è±¡å…±ç”¨æ ‡ç­¾é¡µ
Settings.set_singleton_tab_obj(False)

# ==================== é…ç½®å‚æ•° ====================

SEARCH_CONFIGS = {
    'keywords': ['èƒ½æºç®¡ç†'],
    'cities': {
        'åŒ—äº¬': '101010100',
    }
}

MAX_SCROLLS = 5

# è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE = 'boss_jobs_progress.csv'
PROGRESS_FILE = 'progress_state.txt'

# åæ£€æµ‹é…ç½®
MIN_DELAY = 5
MAX_DELAY = 10
DETAIL_PAGE_DELAY = 8  # è¯¦æƒ…é¡µå»¶è¿Ÿæ›´é•¿ï¼ˆç§’ï¼‰

# ==================== å…¨å±€å˜é‡ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰====================

all_jobs_data = []
processed_count = 0

def signal_handler(sig, frame):
    """å¤„ç†Ctrl+Cï¼Œä¿å­˜å·²é‡‡é›†çš„æ•°æ®"""
    print(f"\n\næ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­...")
    print(f"æ­£åœ¨ä¿å­˜å·²é‡‡é›†çš„ {len(all_jobs_data)} æ¡æ•°æ®...")

    save_data_immediately(all_jobs_data)
    print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print(f"âœ“ å…±ä¿å­˜ {len(all_jobs_data)} æ¡èŒä½æ•°æ®")

    sys.exit(0)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

# ==================== å·¥å…·å‡½æ•° ====================

def random_delay():
    """éšæœºå»¶è¿Ÿ"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

def normalize_company_name(company_name):
    """å…¬å¸åç§°å½’ä¸€åŒ–"""
    if not company_name:
        return 'æœªçŸ¥å…¬å¸'
    company_name = re.sub(r'(æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|è´£ä»»æœ‰é™å…¬å¸|é›†å›¢|ç§‘æŠ€|ç½‘ç»œ|ä¿¡æ¯æŠ€æœ¯|ç”µå­|ç³»ç»Ÿ|é›†æˆ|å‘å±•|æ§è‚¡|æŠ•èµ„|å’¨è¯¢|ç®¡ç†|æœåŠ¡|æ•™è‚²|æ–‡åŒ–|ä¼ åª’|ç¯å¢ƒ|èƒ½æº|ç”µåŠ›|æ–°èƒ½æº|æ™ºèƒ½|æ•°æ®|è½¯ä»¶|å¹³å°)$', '', company_name)
    company_name = re.sub(r'\([^)]*\)', '', company_name)
    company_name = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', company_name)
    company_name = company_name.strip()
    return company_name if company_name else 'æœªçŸ¥å…¬å¸'

def classify_company_type(company_name_raw, nature, scale):
    """åˆ¤æ–­å…¬å¸æ€§è´¨"""
    name = company_name_raw.lower() if company_name_raw else ''

    soe_keywords = ['å›½å®¶ç”µç½‘', 'å—æ–¹ç”µç½‘', 'åèƒ½', 'å¤§å”', 'åç”µ', 'å›½ç”µ', 'ä¸­ç”µæŠ•',
                    'ä¸­çŸ³æ²¹', 'ä¸­çŸ³åŒ–', 'ä¸­æµ·æ²¹', 'å›½å®¶èƒ½æº', 'ä¸­å¹¿æ ¸', 'åæ¶¦',
                    'ä¸­å›½ç”µä¿¡', 'ä¸­å›½ç§»åŠ¨', 'ä¸­å›½è”é€š', 'ä¸­é“', 'ä¸­å»º', 'ä¸­äº¤',
                    'ä¸‰å³¡é›†å›¢', 'ä¸­æ ¸', 'ä¸­èˆª', 'èˆªå¤©ç§‘å·¥', 'èˆªå¤©ç§‘æŠ€',
                    'ç”µåŠ›å…¬å¸', 'èƒ½æºé›†å›¢', 'å‘ç”µé›†å›¢', 'ç”µç½‘å…¬å¸',
                    'ç ”ç©¶é™¢', 'è®¾è®¡é™¢', 'ç ”ç©¶æ‰€', 'ç§‘å­¦é™¢', 'å¤§å­¦',
                    'é›†å›¢', 'è‚¡ä»½', 'å›½æŠ•', 'ç”µæŠ•', 'èƒ½æº']

    foreign_keywords = ['ç‰¹æ–¯æ‹‰', 'å®é©¬', 'å¥”é©°', 'å¤§ä¼—', 'è¥¿é—¨å­', 'æ–½è€å¾·', 'abb',
                       'é€šç”¨ç”µæ°”', 'éœå°¼éŸ¦å°”', 'è‰¾é»˜ç”Ÿ', 'ä¸‰è±', 'ä¸œèŠ', 'æ—¥ç«‹',
                       'lg', 'ä¸‰æ˜Ÿ', 'sk', 'ç°ä»£', 'å¾®è½¯', 'è°·æ­Œ', 'äºšé©¬é€Š',
                       'è‹¹æœ', 'è‹±ç‰¹å°”', 'amd', 'è‹±ä¼Ÿè¾¾', 'é«˜é€š', 'åšä¸–',
                       'ï¼ˆä¸­å›½ï¼‰', '(ä¸­å›½)', 'ï¼ˆä¸Šæµ·ï¼‰', '(ä¸Šæµ·)']

    startup_keywords = ['åˆ›ä¸š', 'å¤©ä½¿', 'å­µåŒ–', 'ç§‘æŠ€', 'æ™ºèƒ½', 'æ–°èƒ½æºç§‘æŠ€',
                       'æœ‰é™åˆä¼™', 'å·¥ä½œå®¤']

    for keyword in soe_keywords:
        if keyword in name:
            return 'å¤®å›½ä¼/äº‹ä¸šå•ä½'

    for keyword in foreign_keywords:
        if keyword in name:
            return 'å¤–ä¼/åˆèµ„'

    for keyword in startup_keywords:
        if keyword in name:
            return 'åˆåˆ›/åˆ›ä¸šå…¬å¸'

    if nature:
        if 'Aè½®' in nature or 'Bè½®' in nature or 'Cè½®' in nature or 'Dè½®' in nature:
            return 'åˆåˆ›/åˆ›ä¸šå…¬å¸'
        elif 'ä¸Šå¸‚' in nature or 'IPO' in nature:
            return 'æ°‘è¥/ä¸Šå¸‚/å¤§å‹ä¼ä¸š'
        elif 'ä¸éœ€è¦èèµ„' in nature or 'å·²ä¸Šå¸‚' in nature:
            return 'æ°‘è¥/ä¸Šå¸‚/å¤§å‹ä¼ä¸š'

    if scale and ('10000äººä»¥ä¸Š' in scale or '500-9999äºº' in scale or '1000-9999äºº' in scale):
        return 'æ°‘è¥/ä¸Šå¸‚/å¤§å‹ä¼ä¸š'

    return 'å…¶ä»–/ä¸ç¡®å®š'

def parse_salary(salary_text):
    """è§£æè–ªèµ„"""
    if not salary_text or salary_text == 'é¢è®®':
        return (None, None, None, None, 'é¢è®®')

    notes = ''
    salary_months_match = re.search(r'(\d+)è–ª', salary_text)
    if salary_months_match:
        salary_months = int(salary_months_match.group(1))
    else:
        salary_months = 12
        notes = 'é»˜è®¤12è–ª'

    salary_pattern = r'(\d+\.?\d*)[kKä¸‡]-?(\d+\.?\d*)[kKä¸‡]?'
    salary_match = re.search(salary_pattern, salary_text)

    if not salary_match:
        return (salary_months, None, None, None, f'æ— æ³•è§£æ: {salary_text}')

    min_salary = float(salary_match.group(1))
    max_salary = float(salary_match.group(2))

    if 'ä¸‡' in salary_text[:salary_match.end()]:
        min_salary *= 10
        max_salary *= 10

    min_year = int(min_salary * 1000 * salary_months)
    max_year = int(max_salary * 1000 * salary_months)
    avg_year = (min_year + max_year) // 2

    return (salary_months, min_year, max_year, avg_year, notes)

def save_data_immediately(jobs_data):
    """ç«‹å³ä¿å­˜æ•°æ®ï¼ˆè¾¹é‡‡é›†è¾¹ä¿å­˜ï¼‰"""
    if not jobs_data:
        return

    # å¤„ç†æ•°æ®
    seen = set()
    unique_jobs = []

    for job in jobs_data:
        company_std = normalize_company_name(job['company_name_raw'])
        dedup_key = f"{company_std}_{job['job_title']}_{job['city']}"

        if dedup_key not in seen:
            seen.add(dedup_key)

            job['company_name_std'] = company_std
            job['company_type'] = classify_company_type(
                job['company_name_raw'],
                job.get('_raw_nature', ''),
                job.get('_raw_scale', '')
            )

            salary_months, min_year, max_year, avg_year, notes = parse_salary(job['salary_text_raw'])
            job['salary_months'] = salary_months
            job['salary_min_year_rmb'] = min_year
            job['salary_max_year_rmb'] = max_year
            job['salary_avg_year_rmb'] = avg_year

            job_notes = []
            if notes:
                job_notes.append(notes)
            if not job.get('jd_text'):
                job_notes.append('æ— èŒä½æè¿°')
            if job['company_type'] == 'å…¶ä»–/ä¸ç¡®å®š':
                job_notes.append('å…¬å¸æ€§è´¨ä¸ç¡®å®š')

            job['notes'] = '; '.join(job_notes) if job_notes else ''
            job['keyword_group'] = job['keyword']

            unique_jobs.append(job)

    # ä¿å­˜åˆ°CSV
    fieldnames = [
        'keyword_group', 'search_keyword', 'city', 'job_title',
        'company_name_raw', 'company_name_std', 'company_type',
        'salary_text_raw', 'salary_months', 'salary_min_year_rmb',
        'salary_max_year_rmb', 'salary_avg_year_rmb',
        'exp_req', 'edu_req', 'jd_text', 'post_date', 'source_url',
        'collected_at', 'notes'
    ]

    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    temp_file = OUTPUT_FILE + '.tmp'
    with open(temp_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for job in unique_jobs:
            writer.writerow({k: job.get(k, '') for k in fieldnames})

    # é‡å‘½åä¸ºæ­£å¼æ–‡ä»¶
    if os.path.exists(temp_file):
        if os.path.exists(OUTPUT_FILE):
            os.remove(OUTPUT_FILE)
        os.rename(temp_file, OUTPUT_FILE)

# ==================== ä½¿ç”¨APIè·å–è¯¦æƒ…ï¼ˆä¸è·³è½¬é¡µé¢ï¼‰====================

import json

def get_job_detail_api(dp, job_id, security_id, lid):
    try:
        dp.listen.start('zpgeek/job/detail/info.json')

        detail_url = (
            "https://www.zhipin.com/wapi/zpgeek/job/detail/info.json"
            f"?jobId={job_id}&securityId={security_id}&lid={lid}"
        )

        dp.run_js(f'''
        fetch("{detail_url}", {{
            method: "GET",
            credentials: "include",
            headers: {{
                "accept": "application/json",
                "x-requested-with": "XMLHttpRequest"
            }}
        }});
        ''')

        r = dp.listen.wait(timeout=10)
        if not r or not r.response:
            return ''

        body = r.response.body

        # å…³é”®ï¼šæŠŠ body ç»Ÿä¸€è§£æä¸º dict
        if isinstance(body, (bytes, bytearray)):
            body = body.decode('utf-8', errors='ignore')
        if isinstance(body, str):
            body = json.loads(body)

        if isinstance(body, dict) and body.get('code') == 0 and 'zpData' in body:
            job_detail = body['zpData']
            job_info = (job_detail or {}).get('jobInfo', {}) or {}

            jd_text = (
                job_info.get('jobDescription', '') or
                job_info.get('positionRemark', '') or
                (job_info.get('responsibility', '') + job_info.get('requirement', ''))
            )
            return jd_text.strip()

        return ''

    except Exception as e:
        print(f"    âš  APIè·å–è¯¦æƒ…å¤±è´¥: {e}")
        return ''


def get_job_detail_click(dp, job_id, security_id, lid):
    """
    å¤‡ç”¨æ–¹æ¡ˆï¼šç‚¹å‡»èŒä½ååœ¨å³ä¾§å¼¹çª—æŸ¥çœ‹è¯¦æƒ…

    è¿™ç§æ–¹å¼ä¹Ÿä¸éœ€è¦è·³è½¬é¡µé¢
    """
    try:
        # æ‰¾åˆ°èŒä½å¡ç‰‡å¹¶ç‚¹å‡»
        job_card = dp.ele(f'xpath://a[contains(@href, "{job_id}")]')
        if job_card:
            # ç‚¹å‡»èŒä½
            dp.actions.click(job_card)
            time.sleep(3)

            # ä»å³ä¾§å¼¹çª—è·å–è¯¦æƒ…
            detail_panel = dp.ele('css:.job-detail-container') or dp.ele('css:.job-detail-box')
            if detail_panel:
                return detail_panel.text

        return ''

    except Exception as e:
        print(f"    âš  ç‚¹å‡»è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return ''

# ==================== ä¸»é‡‡é›†å‡½æ•° ====================

def collect_jobs_improved(keyword, city_name, city_code):
    """æ”¹è¿›çš„é‡‡é›†å‡½æ•°"""
    global all_jobs_data

    print(f"\n{'='*70}")
    print(f"æ­£åœ¨é‡‡é›†: {city_name} - {keyword}")
    print(f"{'='*70}")

    dp = ChromiumPage()

    # å…ˆè®¿é—®é¦–é¡µ
    print("è®¿é—®BOSSç›´è˜é¦–é¡µ...")
    dp.get('https://www.zhipin.com/')
    time.sleep(3)

    # è®¿é—®æœç´¢é¡µé¢
    search_url = f'https://www.zhipin.com/web/geek/job?query={keyword}&city={city_code}'
    print(f"è®¿é—®æœç´¢é¡µé¢: {search_url}")
    dp.get(search_url)

    print("\nâ³ ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆ30ç§’ï¼‰...")
    print("æç¤ºï¼šè¯·æ‰‹åŠ¨å®ŒæˆäººæœºéªŒè¯å’Œç™»å½•")

    for i in range(30, 0, -5):
        print(f"  å€’è®¡æ—¶: {i} ç§’", end='\r')
        time.sleep(5)
    print("\n")

    # æ£€æŸ¥æ˜¯å¦è¢«å°
    page_text = dp.html.lower()
    if 'å¼‚å¸¸' in page_text or 'ç¦æ­¢' in page_text or 'è´¦å·å­˜åœ¨å¼‚å¸¸' in page_text:
        print("âŒ æ£€æµ‹åˆ°è´¦å·å¼‚å¸¸æˆ–è¢«å°ç¦")
        dp.quit()
        return []

    jobs_data = []
    processed_job_ids = set()

    # é˜¶æ®µ1ï¼šæ»šåŠ¨æ”¶é›†èŒä½åˆ—è¡¨
    print(f"\nå¼€å§‹æ»šåŠ¨æ”¶é›†ï¼ˆæœ€å¤š {MAX_SCROLLS} æ¬¡ï¼‰...")

    for scroll_count in range(1, MAX_SCROLLS + 1):
        print(f'\nç¬¬ {scroll_count} æ¬¡æ»šåŠ¨')

        try:
            dp.listen.start('zpgeek/search/joblist.json')

            # æ»šåŠ¨
            scroll_times = random.randint(2, 4)
            for i in range(scroll_times):
                scroll_distance = random.randint(300, 600)
                dp.scroll.down(scroll_distance)
                time.sleep(random.uniform(0.3, 0.8))

            random_delay()
            dp.scroll.to_bottom()
            time.sleep(random.uniform(2, 4))

            # ç­‰å¾…API
            r = dp.listen.wait(timeout=15)
            if not r:
                print("  âš  æœªæ•è·åˆ°APIå“åº”")
                random_delay()
                continue

            json_data = r.response.body
            if 'zpData' not in json_data or 'jobList' not in json_data['zpData']:
                print("  âš  APIå“åº”æ ¼å¼å¼‚å¸¸")
                random_delay()
                continue

            jobList = json_data['zpData']['jobList']
            new_jobs = 0

            for job in jobList:
                job_id = job.get('encryptJobId', '')

                if job_id in processed_job_ids:
                    continue

                processed_job_ids.add(job_id)
                new_jobs += 1

                job_info = {
                    'keyword': keyword,
                    'search_keyword': keyword,
                    'city': city_name,
                    'job_title': job.get('jobName', ''),
                    'company_name_raw': job.get('brandName', ''),
                    'salary_text_raw': job.get('salaryDesc', ''),
                    'exp_req': job.get('jobExperience', ''),
                    'edu_req': job.get('jobDegree', ''),
                    'post_date': job.get('lastUpdateDate', ''),
                    'source_url': f"https://www.zhipin.com/job_detail/{job_id}.html",
                    'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    '_raw_nature': job.get('brandStageName', ''),
                    '_raw_scale': job.get('brandScaleName', ''),
                    '_raw_district': job.get('areaDistrict', ''),
                    '_raw_business': job.get('businessDistrict', ''),
                    '_raw_industry': job.get('brandIndustry', ''),
                    '_raw_skills': ' '.join(job.get('skills', [])),
                    '_raw_welfare': ' '.join(job.get('welfareList', [])),
                    '_security_id': job.get('securityId', ''),
                    '_lid': json_data.get('zpData', {}).get('lid', ''),
                    '_job_id': job_id
                }

                jobs_data.append(job_info)
                print(f"  âœ“ [{len(jobs_data)}] {job_info['job_title'][:25]} | {job_info['company_name_raw'][:20]}")

            print(f"  æœ¬æ¬¡æ–°å¢: {new_jobs}, ç´¯è®¡: {len(jobs_data)}")

            # æ¯æ¬¡æ»šåŠ¨åç«‹å³ä¿å­˜
            if jobs_data:
                all_jobs_data.extend(jobs_data)
                save_data_immediately(all_jobs_data)
                jobs_data = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨

            if new_jobs == 0 and scroll_count >= 2:
                print("  æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢æ»šåŠ¨")
                break

            random_delay()

        except Exception as e:
            print(f"  âœ— å‡ºé”™: {e}")
            random_delay()
            continue

    # é˜¶æ®µ2ï¼šè·å–èŒä½è¯¦æƒ…ï¼ˆä½¿ç”¨APIï¼Œä¸è·³è½¬ï¼‰
    print(f"\nå¼€å§‹è·å–èŒä½è¯¦æƒ…ï¼ˆä½¿ç”¨APIï¼Œé¿å…è·³è½¬ï¼‰...")

    all_jobs_with_details = []

    for idx, job in enumerate(all_jobs_data):
        try:
            job_id = job.get('_job_id', '')
            security_id = job.get('_security_id', '')
            lid = job.get('_lid', '')

            # å…ˆå°è¯•APIæ–¹å¼
            jd_text = get_job_detail_api(dp, job_id, security_id, lid)

            # å¦‚æœAPIå¤±è´¥ï¼Œå°è¯•ç‚¹å‡»æ–¹å¼
            if not jd_text:
                jd_text = get_job_detail_click(dp, job_id, security_id, lid)

            job['jd_text'] = jd_text if jd_text else ''

            all_jobs_with_details.append(job)

            status = 'âœ“ æœ‰æè¿°' if jd_text else 'âœ— æ— æè¿°'
            print(f"  [{idx+1}/{len(all_jobs_data)}] {job['job_title'][:25]} | {status}")

            # æ¯è·å–5ä¸ªè¯¦æƒ…å°±ä¿å­˜ä¸€æ¬¡
            if (idx + 1) % 5 == 0:
                save_data_immediately(all_jobs_with_details)
                print(f"    ğŸ’¾ å·²ä¿å­˜ {len(all_jobs_with_details)} æ¡æ•°æ®")

            # æ›´é•¿çš„å»¶è¿Ÿï¼Œé¿å…è§¦å‘æ£€æµ‹
            delay = random.uniform(DETAIL_PAGE_DELAY - 2, DETAIL_PAGE_DELAY + 2)
            print(f"    ç­‰å¾… {delay:.1f} ç§’...")
            time.sleep(delay)

        except Exception as e:
            print(f"  âœ— å¤„ç†èŒä½ {idx+1} å‡ºé”™: {e}")
            all_jobs_with_details.append(job)  # å³ä½¿å‡ºé”™ä¹Ÿä¿ç•™
            continue

    dp.quit()
    print(f"\nâœ“ é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(all_jobs_with_details)} æ¡èŒä½æ•°æ®")

    return all_jobs_with_details

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("BOSSç›´è˜æ‰¹é‡æ•°æ®é‡‡é›†å·¥å…·ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    print("="*70)
    print(f"\næ”¹è¿›ç‚¹ï¼š")
    print(f"  1. è¾¹é‡‡é›†è¾¹ä¿å­˜ï¼ŒCtrl+Cä¸ä¼šä¸¢å¤±æ•°æ®")
    print(f"  2. ä½¿ç”¨APIè·å–è¯¦æƒ…ï¼Œé¿å…è·³è½¬é¡µé¢")
    print(f"  3. æ›´é•¿çš„å»¶è¿Ÿæ—¶é—´ï¼ˆ5-10ç§’ï¼‰")
    print(f"  4. å®æ—¶ä¿å­˜è¿›åº¦")

    print(f"\nå½“å‰é…ç½®:")
    print(f"- å…³é”®è¯: {', '.join(SEARCH_CONFIGS['keywords'])}")
    print(f"- åŸå¸‚: {', '.join(SEARCH_CONFIGS['cities'].keys())}")
    print(f"- æ»šåŠ¨æ¬¡æ•°: {MAX_SCROLLS}")

    input("\næŒ‰Enteré”®å¼€å§‹é‡‡é›†...")

    for keyword in SEARCH_CONFIGS['keywords']:
        for city_name, city_code in SEARCH_CONFIGS['cities'].items():
            try:
                jobs = collect_jobs_improved(keyword, city_name, city_code)

                # ä¿å­˜æœ€ç»ˆæ•°æ®
                if jobs:
                    save_data_immediately(jobs)
                    print(f"\nâœ“ {city_name}-{keyword} æ•°æ®å·²ä¿å­˜")

                time.sleep(10)

            except Exception as e:
                print(f"âœ— é‡‡é›†å¤±è´¥: {city_name} - {keyword}, é”™è¯¯: {e}")
                continue

    print(f"\n{'='*70}")
    print("å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"\nâœ“ æœ€ç»ˆæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"âœ“ å…±ä¿å­˜: {len(all_jobs_data)} æ¡èŒä½æ•°æ®")

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        signal_handler(None, None)
    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
